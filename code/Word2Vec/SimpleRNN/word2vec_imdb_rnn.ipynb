{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 00:39:09.036606: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-29 00:39:09.036705: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-29 00:39:09.036763: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-29 00:39:09.036817: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-29 00:39:09.036854: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, GRU, Bidirectional, GlobalMaxPooling1D,GlobalAveragePooling1D, Dropout,SpatialDropout1D,Conv1D,concatenate,Flatten, SimpleRNN, LSTM\n",
    "#Bidirectional은 양방향으로 데이터를 보기 위해서 사용되는 함수\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 불러오기\n",
    "data = pd.read_csv('/home/hsung/ds_study/Vit/IMDB Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(len(data)*0.7)\n",
    "train_data = data[:cutoff]\n",
    "test_data = data[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading stop words\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "# removing 'not' word from stop words list because this word has enough effect on sentiment\n",
    "nlp.Defaults.stop_words.remove('not')\n",
    "nlp.vocab['not'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    \n",
    "    # remove stop words\n",
    "    raw = []\n",
    "    for word in text.split():\n",
    "        if word in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            raw.append(word)\n",
    "    text = ' '.join(raw)\n",
    "    # lemmatization\n",
    "    text = ' '.join([i.lemma_ for i in nlp(text)])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = train_data['review'].map(lambda com : clean_text(com))\n",
    "X_test_raw = test_data['review'].map(lambda com : clean_text(com))\n",
    "\n",
    "Y_train = train_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape :  (35000,)\n",
      "X test shape :  (15000,)\n",
      "y train shape :  (35000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape : \", X_train_raw.shape)\n",
    "print(\"X test shape : \", X_test_raw.shape)\n",
    "print(\"y train shape : \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=90000\n",
    "maxlen=90\n",
    "embed_size=300\n",
    "\n",
    "\n",
    "tok=Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(X_train_raw)\n",
    "tok.fit_on_texts(X_test_raw)\n",
    "X_train = sequence.pad_sequences(tok.texts_to_sequences(X_train_raw),\n",
    "                        maxlen = maxlen, truncating = \"pre\")\n",
    "X_test = sequence.pad_sequences(tok.texts_to_sequences(X_test_raw),\n",
    "                       maxlen = maxlen, truncating = \"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {e:i for e,i in word_index.items() if i <= max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 90)   (31500,)   (3500, 90)   (3500,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X_train,Y_train,train_size=0.9, random_state=199)\n",
    "print(x_train.shape,\" \",y_train.shape,\" \",x_val.shape,\" \",y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 크기(shape) : (3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/home/hsung/ds_study/Vit/New/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "print('모델의 크기(shape) :',word2vec_model.vectors.shape) # 모델의 크기 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 행렬의 크기(shape) : (90000, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((max_features, 300))\n",
    "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in word_index.items():\n",
    "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
    "    vector_value = get_vector(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index-1] = vector_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 02:00:36.910401: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 90)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 90, 300)           27000000  \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 128)               54912     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                6450      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,061,464\n",
      "Trainable params: 27,061,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(maxlen, )) \n",
    "layer = Embedding(max_features, 300, weights=[embedding_matrix], input_length=maxlen, trainable=True)(input)\n",
    "layer = SimpleRNN(128)(layer)   \n",
    "layer = Dense(50,activation='relu')(layer)\n",
    "layer = Flatten()(layer)\n",
    "layer= Dense(2, activation=\"sigmoid\")(layer)\n",
    "model = Model(inputs = input, outputs = layer)  \n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '{epoch:02d}-{val_loss:.5f}.h5'\n",
    "checkpoint = ModelCheckpoint(file_path,monitor='val_loss',verbose = 1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train=='positive'] = 1\n",
    "y_train[y_train=='negative'] = 0\n",
    "y_val[y_val=='positive'] = 1\n",
    "y_val[y_val=='negative'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical \n",
    "y_train_one = to_categorical(y_train)\n",
    "y_val_one = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5598 - accuracy: 0.6886\n",
      "Epoch 1: val_loss improved from inf to 0.42581, saving model to 01-0.42581.h5\n",
      "124/124 [==============================] - 31s 247ms/step - loss: 0.5598 - accuracy: 0.6886 - val_loss: 0.4258 - val_accuracy: 0.8140\n",
      "Epoch 2/3\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.8902\n",
      "Epoch 2: val_loss improved from 0.42581 to 0.39563, saving model to 02-0.39563.h5\n",
      "124/124 [==============================] - 29s 233ms/step - loss: 0.2744 - accuracy: 0.8902 - val_loss: 0.3956 - val_accuracy: 0.8200\n",
      "Epoch 3/3\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9499\n",
      "Epoch 3: val_loss did not improve from 0.39563\n",
      "124/124 [==============================] - 29s 233ms/step - loss: 0.1424 - accuracy: 0.9499 - val_loss: 0.4227 - val_accuracy: 0.8380\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train_one, epochs=3,batch_size=256, validation_data=(x_val,y_val_one), callbacks= [checkpoint,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def acc_loss_plot(hist):\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc = 'upper left')\n",
    "\n",
    "    acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "    acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "    acc_ax.set_ylabel('accuracy')\n",
    "    acc_ax.legend(loc='upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data['sentiment'] \n",
    "y_test[y_test=='positive'] = 1\n",
    "y_test[y_test=='negative'] = 0\n",
    "y_test_one = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 6s 13ms/step - loss: 0.4260 - accuracy: 0.8430\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test,y_test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8429999947547913"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hsung_virconda': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "601d87b01c3bfe1f8b2d58089fe34fc1d031227df5f23ff1d0566eb6d404eeeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
