{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvrEBKrG8gW2",
        "outputId": "822fe04f-0107-4952-81e4-e734aca19392"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pwDJA7uY7JUM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn60vaxd8CLJ",
        "outputId": "902e50c4-ef5f-4c00-f69f-ee4a7e69ae6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('/home/hsung/Downloads/workspace/IMDB Dataset.csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypVHc6mQCQlM",
        "outputId": "de6a7fe0-97a9-46ad-fded-f2a5a8b3ff17"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AF5Asgl58v0i",
        "outputId": "07c82bb9-c40d-45a0-fe57-7ee74d70ae13"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      One of the other reviewers has mentioned that ...  positive\n",
              "1      A wonderful little production. <br /><br />The...  positive\n",
              "2      I thought this was a wonderful way to spend ti...  positive\n",
              "3      Basically there's a family where a little boy ...  negative\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...  positive\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  I am a Catholic taught in parochial elementary...  negative\n",
              "49998  I'm going to have to disagree with the previou...  negative\n",
              "49999  No one expects the Star Trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o7-kA7-K8x43"
      },
      "outputs": [],
      "source": [
        "def convert_to_sentiment(x):\n",
        "  if x == 'positive':\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bat9ew0a869I"
      },
      "outputs": [],
      "source": [
        "df['sentiment'] = df['sentiment'].apply(convert_to_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L4Yc4mGQ9BjU",
        "outputId": "cb63cdbe-5949-4772-dfce-1e10918b28bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production. <br /><br />The...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "8TD4bqES9DsU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(df['review'],df['sentiment'],test_size=0.3,random_state=12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zfnkvsk9RiO",
        "outputId": "69f4ecff-532f-468d-bea4-f6d680d4e55d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[11, 19, 13, 249, 5, 74, 3, 1075, 4, 2, 51, 10, 852, 215, 9, 1, 1419, 13, 4545, 10, 385, 11, 6, 27, 4, 1, 77, 569, 4, 1, 8947, 1036, 32, 1723, 128, 296, 3084, 4, 1, 6542, 12850, 1562, 2, 216, 242, 17, 3256, 8, 10741, 210, 10227, 254, 38, 18561, 1, 594, 17, 29, 4, 11, 2121, 8, 622, 85, 98, 254, 94, 138, 3, 5120, 703, 8243, 490, 4, 11, 292, 114, 1566, 136, 1799, 6, 217, 219, 1, 6067, 5029, 12, 1, 1092, 499, 3192, 3650, 13, 17872, 3727, 36, 3, 15, 324, 582, 48, 73, 74, 6, 3, 530, 210, 99, 1840, 1331, 38, 6655, 82, 1, 323, 4, 3, 404, 3651, 195, 2, 1055, 195, 5, 25, 2037, 9, 123, 24423, 2380, 2280, 1, 15, 17, 40, 17296, 2, 1072, 932, 18, 1, 143, 8139, 13, 8, 1, 5637, 4, 1, 112, 2, 1, 868, 466, 856, 20, 11, 19, 16, 56, 6602], [262, 340, 1, 1241, 2, 106, 1, 5288, 1981, 3476, 1262, 7, 7, 6007, 316, 10, 25, 5, 131, 12, 11, 6, 21, 49, 30, 29, 7, 7, 1843, 5, 1, 1241, 14955, 6, 450, 5, 26, 3, 1456, 7, 7, 32575, 11327, 51, 229, 109, 14955, 2, 1889, 2, 1055, 7, 7, 51, 229, 109, 1, 17873, 18, 133, 28, 39, 283, 1080, 14, 7, 7, 14955, 2, 385, 14, 1, 17873, 15802, 6, 450, 5, 26, 7, 7, 1, 88, 308, 255, 8, 2149, 21, 3, 1472, 2, 23128, 258, 21993, 59, 283, 661, 2243, 236, 705, 5, 7, 7, 76, 94, 53, 826, 943, 2515, 45, 225, 72, 126, 178, 2, 7, 7, 2755, 36037, 111, 6, 3399, 2, 1032, 1, 2285, 2, 7, 7, 8024, 4, 1, 1241, 2, 1, 3476, 6007, 19, 6, 133, 3117, 7, 7, 31, 27703, 2, 6390, 797, 12, 39, 89, 94, 1, 577, 7, 7, 10, 235, 715, 12, 5, 3, 404, 34, 1600, 106, 99, 918, 316, 7, 7, 38, 340, 1, 275, 9, 58, 236, 163, 573], [2, 10, 1220, 3, 1198, 5761, 16752, 29, 121, 12, 348, 36, 3, 1952, 17, 24, 40825, 3102, 294, 47, 14546, 104, 23, 181, 735, 36, 3214, 1635, 24424, 61439, 47, 14546, 104, 23, 39, 76, 36, 7497, 40826, 20141, 508, 18, 14546, 104, 12, 358, 2, 78, 1, 1153, 20142, 148, 4, 109, 17297, 76, 141, 217, 26, 6801, 17, 1512, 8244, 3, 169, 4, 83, 101, 24425, 2177, 25881, 6, 3, 76, 1255, 924, 16, 472, 18, 145, 3118, 254, 121, 1, 624, 4, 17298, 81, 20143, 9, 123, 14, 3728, 4946, 93, 220, 317, 5465, 454, 324, 421, 44, 8948, 2, 413, 3, 2255, 4, 91, 55, 18, 14, 3, 19, 44, 61, 105, 18562, 721, 3728, 11, 54, 327, 706, 318, 206, 180, 5507, 518, 5, 2915, 3, 6161, 1230, 17, 91, 2211, 1741, 5, 91, 1637, 9619, 4326, 1498, 771, 92, 40827, 180, 1441, 2, 179, 72, 230, 31, 5195, 3132, 18, 705, 5, 6656, 369, 31, 336, 3353, 238, 149, 1, 980, 805, 283, 36, 4279, 61440, 3, 227, 12, 713, 5, 25, 75, 411, 20, 1, 140, 4, 3, 23129, 48120, 31, 105, 61441, 17, 54, 1765, 4, 85, 19, 206, 500, 2567, 61442, 4895, 3462, 5663, 754, 733, 2, 353, 1173, 110, 4406, 721, 1516, 1, 5060, 939, 6, 3, 19, 12, 67, 61, 26, 1422, 30, 245, 71, 17, 42, 124, 320, 4, 27704, 6, 36038, 20144, 1694, 61443, 439, 2125, 792, 2, 1741, 5, 191, 1494, 238, 6872, 471, 350, 11, 27, 18, 89, 522, 96, 72], [12584, 14164, 45, 1014, 1, 126, 173, 4, 40, 1153, 25882, 616, 32576, 174, 17, 32, 2713, 6930, 111, 243, 25883, 8, 198, 40, 5704, 18563, 2, 9209, 12585, 18, 51, 9, 279, 176, 5, 9, 220, 54, 8713, 12, 11, 253, 67, 487, 222, 37, 3, 65, 4, 1630, 7653, 11, 70, 225, 238, 15, 6, 4038, 2043, 2, 2018, 704, 14164, 303, 1, 13164, 318, 7498, 584, 213, 344, 61, 5, 164, 12, 59, 1070, 1, 239, 479, 4, 40, 10742, 114, 14, 70, 14, 1, 3687, 72, 5, 1, 14956, 4, 40, 50, 7412, 2, 3688, 50, 6162, 820, 3179, 2439, 303, 1, 113, 622, 2, 3, 65, 3901, 17, 29, 1, 831, 823, 4, 27705, 440, 18, 93, 220, 217, 12, 32576, 12584, 5, 103, 2, 59, 125, 855, 2168, 14165, 1702, 2, 421, 10, 975, 9, 10, 116, 25, 12, 12584, 3239, 20, 57, 1581, 95, 140, 51, 12586, 430, 216, 284, 2, 3, 307, 407, 41, 4, 743, 20, 1, 61444, 29837, 58, 26, 2600], [32576, 2483, 595, 37, 3, 1002, 6543, 65, 1041, 31, 317, 13809, 2, 521, 31, 13810, 9740, 826, 5508, 1781, 53, 10056, 19280, 3635, 4573, 5030, 118, 24, 1755, 325, 3447, 17, 535, 12049, 1760, 10915, 14547, 21994, 2, 5508, 314, 216, 1, 29838, 750, 16, 3414, 6657, 5599, 7413, 34, 6, 1458, 40, 114, 140, 294, 100, 1, 326, 4, 40, 669, 2, 534, 48121, 2, 5328, 270, 8, 32, 32577, 55, 2, 273, 2, 17, 3, 608, 2, 2722, 113, 65, 1699, 8, 1, 19281, 27706, 5508, 2, 7413, 78, 25, 47, 49, 1238, 294, 18, 11, 227, 412, 92, 161, 5, 1727, 20, 16, 4947, 32, 532, 88, 4, 1, 408, 3313, 48, 5, 78, 43, 1, 29839, 2423, 8, 1, 3463, 11, 5329, 211, 330, 14, 3943, 245, 44, 3, 791, 12050, 8, 3, 663, 795, 1, 437, 1964, 5, 120, 1, 48122, 12, 13811, 83, 213, 5, 51, 503, 762, 8, 1, 168, 5858, 2, 299, 1286, 64, 653, 6, 203, 18, 475, 1, 21016, 95, 9740, 12587, 11, 15349, 6, 1087, 6163, 14548, 917, 435, 37]]\n",
            "[[43, 251, 5, 366, 44, 19260, 2, 4723, 23, 270, 5, 5343, 7038, 5517, 1403, 2174, 32412, 2456, 38, 46, 77, 200, 746, 18, 2656, 9, 13, 33, 70, 270, 5, 7039, 33, 1210, 3164, 223, 1183, 27, 857, 425, 8, 11, 212, 943, 346, 428, 19260, 36, 1729, 896, 1, 8161, 230, 2, 4723, 25, 55, 1177, 1539, 4723, 187, 5, 26, 270, 15, 3, 2474, 1403, 563, 4, 2095, 210, 444, 13568, 18, 28, 151, 213, 58, 2562, 533, 537, 688, 47, 19260, 196, 28, 13, 389, 18, 112, 150, 247, 297, 9, 28, 187, 5, 101, 12, 16423, 24, 2797, 2, 3165, 3, 169, 2, 4724, 32413, 30, 312, 2, 269, 6, 1, 6179, 4, 253, 200, 266, 15, 83, 9, 6, 18, 21, 15, 1, 299, 10, 385, 315, 11, 8, 1, 2032, 50, 9, 373, 42, 9, 13, 20, 1, 1424, 4, 3, 1390, 986, 16, 3, 257, 303, 154, 2107, 9557, 869, 42045, 4, 1, 2326, 62, 13, 181, 49, 2, 3, 257, 3986, 337, 4384, 32414, 62, 297, 2, 41, 847, 226, 85, 11, 147, 29, 1, 339, 8, 1, 299, 70, 2825, 534, 30, 1, 253, 9, 13, 35, 23635, 2164, 12, 9, 153, 58, 4313, 53, 5, 1, 248, 4, 1, 257, 3986, 2948, 10, 2603, 179, 15, 1, 126, 4, 9, 21, 12, 10, 440, 5, 18, 54, 1614, 1265, 638, 5, 1196, 68, 53, 365, 100, 1, 17, 1026, 2, 31, 1, 56, 11, 944, 13, 121, 10, 13, 1, 60, 27, 8, 1, 726, 10, 203, 9, 173, 41, 1133, 38, 35, 152, 590, 20, 1968, 27, 295, 2, 1643, 179, 5, 104, 9, 5, 63, 44, 9, 13, 14, 74, 14, 10, 1921, 9, 13, 446, 1, 60, 147, 9, 66, 165, 15, 9, 13, 1993, 15341, 36, 13, 14, 42046, 848, 14, 123, 77, 73, 39, 11, 147, 45, 431, 163, 5, 380, 9, 1, 106, 4, 91, 87, 276, 888, 1251, 2, 1, 95, 33, 75, 9, 164, 1899, 17738, 3464, 387, 4, 888, 102, 14, 3900, 23636, 3277, 15342, 42047, 1831, 42048, 162, 17739, 31, 2002, 4723, 419, 20, 5, 2864, 367, 17740, 2, 19260, 818, 270, 24, 494, 30, 228, 96, 18, 88, 4, 91, 70, 214, 14, 74, 14, 11, 21, 174, 149, 14, 10, 89, 101, 237, 97, 26, 174, 14, 74, 14, 11, 3, 384, 1141, 455, 4, 19, 89, 455, 127, 56, 20, 9], [69, 114, 75, 10, 377, 7, 7, 14, 27, 4, 1, 77, 2114, 307, 22, 119, 335, 8, 15, 3, 146, 1772, 50, 22, 63, 1, 642, 329, 226, 2, 226, 4, 19, 56, 931, 20, 3, 215, 1963, 20, 3, 42049, 7, 7, 10, 501, 3013, 23637, 318, 2475, 4, 1, 61, 47, 10, 59, 37, 5, 134, 149, 6, 12, 11, 19, 12881, 68, 51, 73, 98, 19, 10, 67, 385, 10, 1715, 872, 6051, 2, 20, 1, 1329, 4, 54, 2152, 29, 1, 95, 140, 1, 354, 159, 448, 1319, 2, 819, 364, 583, 12, 4, 1536, 14321, 2476, 22, 5, 1, 253, 2, 501, 358, 22, 162, 240, 7, 7, 43, 1077, 35, 4, 268, 312, 294, 819, 18, 9, 6, 31, 55, 792, 1629, 32, 3317, 8, 591, 403, 11, 6, 3, 19, 16, 708, 5, 134, 41, 1944, 1969, 2, 109, 7, 7, 139, 2, 63, 9], [54, 325, 2, 339, 8, 54, 678, 6, 32, 1630, 4550, 350, 10, 793, 108, 167, 410, 18, 10, 130, 530, 9, 48, 23, 105, 699, 12, 10, 530, 27, 4, 91, 13, 114, 4314, 727, 1343, 629, 1523, 194, 24, 17741, 6875, 44, 22, 183, 5, 119, 133, 473, 25, 5, 25, 108, 9, 15, 640, 157, 10, 5518, 11, 53, 481, 37, 5, 134, 12, 312, 202, 513, 3, 49, 232, 1, 356, 1600, 13, 2131, 1, 1298, 70, 69, 2620, 2, 1, 484, 13, 202, 52, 570, 8, 1232, 58, 149, 9, 67, 26, 108, 8, 12882, 145, 10, 2418, 380, 22, 1336, 9, 40, 8, 405, 9, 267, 122, 1, 973, 15, 49], [47, 3, 806, 3, 1190, 23638, 4, 1, 200, 350, 1112, 10671, 42050, 60, 27233, 150, 215, 2011, 19261, 36, 2210, 1, 116, 14, 42051, 8, 2312, 3037, 349, 8, 16424, 6, 2756, 565, 14, 1, 10672, 21112, 795, 11669, 2, 1, 363, 4868, 5170, 7, 7, 1, 27, 129, 12, 6, 64, 150, 6, 50, 795, 32415, 922, 23639, 4442, 11669, 8, 1384, 16, 24, 528, 2, 514, 7, 7, 86, 2781, 22, 186, 3038, 4, 54, 1322, 528, 7, 7, 11669, 5948, 18, 2477, 144, 165, 1474, 16, 1, 4498, 7, 7, 32415, 5948, 143, 417, 69, 8, 12, 405, 2059, 127, 3723, 42], [32416, 3132, 15343, 4080, 11, 1900, 1226, 19, 41, 1226, 367, 2, 1, 1170, 11670, 840, 5, 1, 32417, 4, 3491, 36, 45, 249, 308, 48, 12240, 13569, 1227, 3, 69, 1822, 786, 8, 170, 15, 24, 224, 14, 1, 840, 36, 3822, 6, 175, 8, 3, 225, 170, 8, 3, 1226, 428, 14, 3, 1330, 840, 1, 17, 5171, 143, 5, 24, 504, 8, 3491, 1019, 53, 5, 1, 9558, 808, 5, 16425, 1069, 3359, 164, 24, 201, 253, 1953, 14, 1, 367, 160, 36, 6301, 13569, 8, 24, 19, 1, 19, 2657, 14, 32, 217, 162, 30, 1, 808, 4, 3491, 2, 30, 32, 5078, 4, 512, 1, 137, 32418, 8, 1, 407, 504, 3132, 15343, 1490, 238, 158, 350, 2, 27, 12, 6, 1060, 16, 1, 82, 767, 4, 899, 4315, 2, 1721]]\n"
          ]
        }
      ],
      "source": [
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(df['review'])\n",
        "# X_train_encoded = tokenizer.texts_to_sequences(df['review'])\n",
        "# print(X_train_encoded[:5])\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "print(X_train_encoded[:5])\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_test)\n",
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "print(X_test_encoded[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "UrMu20IF9z6o"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvqrzfyK-Eo4",
        "outputId": "f03ff9ba-067e-4672-b651-b2467253dee3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_padded[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "fcsvkI4h-GqN"
      },
      "outputs": [],
      "source": [
        "x = X_train_padded.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "gyU-8z5D-JVE"
      },
      "outputs": [],
      "source": [
        "#y = df['sentiment'].values\n",
        "\n",
        "y= y_train.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtzZqIqy-Mhy",
        "outputId": "d6174226-fd26-43ad-bfdf-2d585ef11247"
      },
      "outputs": [],
      "source": [
        "#!pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Od0wkwnH-akm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riRTTgmJ-elf",
        "outputId": "0fd277f2-a758-400d-c1bc-668d295d9d1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(35000, 100)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "v9-WS4TV-h6b"
      },
      "outputs": [],
      "source": [
        "x = x.reshape(35000,10,10,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "4DsrRHvg-smr"
      },
      "outputs": [],
      "source": [
        "input_shape = (10,10,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "UQy6S7RY-mnf"
      },
      "outputs": [],
      "source": [
        "patch_size = (5, 5)  # 2-by-2 sized patches\n",
        "dropout_rate = 0.03  # Dropout rate\n",
        "num_heads = 2  # Attention heads\n",
        "embed_dim = 64  # Embedding dimension\n",
        "num_mlp = 256  # MLP layer size\n",
        "qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n",
        "window_size = 2  # Size of attention window\n",
        "shift_size = 1  # Size of shifting window\n",
        "image_dimension = 10  # Initial image size\n",
        "\n",
        "num_patch_x = input_shape[0] // patch_size[0]\n",
        "num_patch_y = input_shape[1] // patch_size[1]\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 128\n",
        "num_epochs = 300\n",
        "validation_split = 0.1\n",
        "weight_decay = 0.0001\n",
        "label_smoothing = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIqP1CLd_IU2",
        "outputId": "6a4d4606-21ea-41cc-8f4a-ae20ce9f1961"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_patch_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FyCKbeMy-uUp"
      },
      "outputs": [],
      "source": [
        "def window_partition(x, window_size):\n",
        "    _, height, width, channels = x.shape\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = tf.reshape(\n",
        "        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n",
        "    )\n",
        "    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n",
        "    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, height, width, channels):\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = tf.reshape(\n",
        "        windows,\n",
        "        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n",
        "    )\n",
        "    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n",
        "    x = tf.reshape(x, shape=(-1, height, width, channels))\n",
        "    return x\n",
        "\n",
        "\n",
        "class DropPath(layers.Layer):\n",
        "    def __init__(self, drop_prob=None, **kwargs):\n",
        "        super(DropPath, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def call(self, x):\n",
        "        input_shape = tf.shape(x)\n",
        "        batch_size = input_shape[0]\n",
        "        rank = x.shape.rank\n",
        "        shape = (batch_size,) + (1,) * (rank - 1)\n",
        "        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n",
        "        path_mask = tf.floor(random_tensor)\n",
        "        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "q9vR1NAk-wUA"
      },
      "outputs": [],
      "source": [
        "class WindowAttention(layers.Layer):\n",
        "    def __init__(\n",
        "        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n",
        "    ):\n",
        "        super(WindowAttention, self).__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.proj = layers.Dense(dim)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        num_window_elements = (2 * self.window_size[0] - 1) * (\n",
        "            2 * self.window_size[1] - 1\n",
        "        )\n",
        "        self.relative_position_bias_table = self.add_weight(\n",
        "            shape=(num_window_elements, self.num_heads),\n",
        "            initializer=tf.initializers.Zeros(),\n",
        "            trainable=True,\n",
        "        )\n",
        "        coords_h = np.arange(self.window_size[0])\n",
        "        coords_w = np.arange(self.window_size[1])\n",
        "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n",
        "        coords = np.stack(coords_matrix)\n",
        "        coords_flatten = coords.reshape(2, -1)\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)\n",
        "\n",
        "        self.relative_position_index = tf.Variable(\n",
        "            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n",
        "        )\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        _, size, channels = x.shape\n",
        "        head_dim = channels // self.num_heads\n",
        "        x_qkv = self.qkv(x)\n",
        "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n",
        "        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n",
        "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
        "        q = q * self.scale\n",
        "        k = tf.transpose(k, perm=(0, 1, 3, 2))\n",
        "        attn = q @ k\n",
        "\n",
        "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
        "        relative_position_index_flat = tf.reshape(\n",
        "            self.relative_position_index, shape=(-1,)\n",
        "        )\n",
        "        relative_position_bias = tf.gather(\n",
        "            self.relative_position_bias_table, relative_position_index_flat\n",
        "        )\n",
        "        relative_position_bias = tf.reshape(\n",
        "            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n",
        "        )\n",
        "        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n",
        "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.get_shape()[0]\n",
        "            mask_float = tf.cast(\n",
        "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n",
        "            )\n",
        "            attn = (\n",
        "                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n",
        "                + mask_float\n",
        "            )\n",
        "            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n",
        "            attn = keras.activations.softmax(attn, axis=-1)\n",
        "        else:\n",
        "            attn = keras.activations.softmax(attn, axis=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x_qkv = attn @ v\n",
        "        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n",
        "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n",
        "        x_qkv = self.proj(x_qkv)\n",
        "        x_qkv = self.dropout(x_qkv)\n",
        "        return x_qkv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "_2XqhkiL-y4x"
      },
      "outputs": [],
      "source": [
        "class SwinTransformer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_patch,\n",
        "        num_heads,\n",
        "        window_size=7,\n",
        "        shift_size=0,\n",
        "        num_mlp=1024,\n",
        "        qkv_bias=True,\n",
        "        dropout_rate=0.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(SwinTransformer, self).__init__(**kwargs)\n",
        "\n",
        "        self.dim = dim  # number of input dimensions\n",
        "        self.num_patch = num_patch  # number of embedded patches\n",
        "        self.num_heads = num_heads  # number of attention heads\n",
        "        self.window_size = window_size  # size of window\n",
        "        self.shift_size = shift_size  # size of window shift\n",
        "        self.num_mlp = num_mlp  # number of MLP nodes\n",
        "\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.attn = WindowAttention(\n",
        "            dim,\n",
        "            window_size=(self.window_size, self.window_size),\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        self.drop_path = DropPath(dropout_rate)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
        "\n",
        "        self.mlp = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(num_mlp),\n",
        "                layers.Activation(keras.activations.gelu),\n",
        "                layers.Dropout(dropout_rate),\n",
        "                layers.Dense(dim),\n",
        "                layers.Dropout(dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if min(self.num_patch) < self.window_size:\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.num_patch)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.shift_size == 0:\n",
        "            self.attn_mask = None\n",
        "        else:\n",
        "            height, width = self.num_patch\n",
        "            h_slices = (\n",
        "                slice(0, -self.window_size),\n",
        "                slice(-self.window_size, -self.shift_size),\n",
        "                slice(-self.shift_size, None),\n",
        "            )\n",
        "            w_slices = (\n",
        "                slice(0, -self.window_size),\n",
        "                slice(-self.window_size, -self.shift_size),\n",
        "                slice(-self.shift_size, None),\n",
        "            )\n",
        "            mask_array = np.zeros((1, height, width, 1))\n",
        "            count = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    mask_array[:, h, w, :] = count\n",
        "                    count += 1\n",
        "            mask_array = tf.convert_to_tensor(mask_array)\n",
        "\n",
        "            # mask array to windows\n",
        "            mask_windows = window_partition(mask_array, self.window_size)\n",
        "            mask_windows = tf.reshape(\n",
        "                mask_windows, shape=[-1, self.window_size * self.window_size]\n",
        "            )\n",
        "            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n",
        "                mask_windows, axis=2\n",
        "            )\n",
        "            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n",
        "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
        "            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n",
        "\n",
        "    def call(self, x):\n",
        "        height, width = self.num_patch\n",
        "        _, num_patches_before, channels = x.shape\n",
        "        x_skip = x\n",
        "        x = self.norm1(x)\n",
        "        x = tf.reshape(x, shape=(-1, height, width, channels))\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = tf.roll(\n",
        "                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n",
        "            )\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        x_windows = window_partition(shifted_x, self.window_size)\n",
        "        x_windows = tf.reshape(\n",
        "            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n",
        "        )\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
        "\n",
        "        attn_windows = tf.reshape(\n",
        "            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n",
        "        )\n",
        "        shifted_x = window_reverse(\n",
        "            attn_windows, self.window_size, height, width, channels\n",
        "        )\n",
        "        if self.shift_size > 0:\n",
        "            x = tf.roll(\n",
        "                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n",
        "            )\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        x = tf.reshape(x, shape=(-1, height * width, channels))\n",
        "        x = self.drop_path(x)\n",
        "        x = x_skip + x\n",
        "        x_skip = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop_path(x)\n",
        "        x = x_skip + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "O6P7qq4O-2AW"
      },
      "outputs": [],
      "source": [
        "class PatchExtract(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super(PatchExtract, self).__init__(**kwargs)\n",
        "        self.patch_size_x = patch_size[0]\n",
        "        self.patch_size_y = patch_size[0]\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n",
        "            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n",
        "            rates=(1, 1, 1, 1),\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dim = patches.shape[-1]\n",
        "        patch_num = patches.shape[1]\n",
        "        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
        "\n",
        "\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
        "        super(PatchEmbedding, self).__init__(**kwargs)\n",
        "        self.num_patch = num_patch\n",
        "        self.proj = layers.Dense(embed_dim)\n",
        "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, patch):\n",
        "        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n",
        "        return self.proj(patch) + self.pos_embed(pos)\n",
        "\n",
        "\n",
        "class PatchMerging(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim):\n",
        "        super(PatchMerging, self).__init__()\n",
        "        self.num_patch = num_patch\n",
        "        self.embed_dim = embed_dim\n",
        "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
        "\n",
        "    def call(self, x):\n",
        "        height, width = self.num_patch\n",
        "        _, _, C = x.get_shape().as_list()\n",
        "        x = tf.reshape(x, shape=(-1, height, width, C))\n",
        "        x0 = x[:, 0::2, 0::2, :]\n",
        "        x1 = x[:, 1::2, 0::2, :]\n",
        "        x2 = x[:, 0::2, 1::2, :]\n",
        "        x3 = x[:, 1::2, 1::2, :]\n",
        "        x = tf.concat((x0, x1, x2, x3), axis=-1)\n",
        "        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n",
        "        return self.linear_trans(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "NCT4F5sN-49X"
      },
      "outputs": [],
      "source": [
        "input = layers.Input(input_shape)\n",
        "x = layers.RandomCrop(image_dimension, image_dimension)(input)\n",
        "x = layers.RandomFlip(\"horizontal\")(x)\n",
        "x = PatchExtract(patch_size)(x)\n",
        "x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n",
        "x = SwinTransformer(\n",
        "    dim=embed_dim,\n",
        "    num_patch=(num_patch_x, num_patch_y),\n",
        "    num_heads=num_heads,\n",
        "    window_size=window_size,\n",
        "    shift_size=0,\n",
        "    num_mlp=num_mlp,\n",
        "    qkv_bias=qkv_bias,\n",
        "    dropout_rate=dropout_rate,\n",
        ")(x)\n",
        "x = SwinTransformer(\n",
        "    dim=embed_dim,\n",
        "    num_patch=(num_patch_x, num_patch_y),\n",
        "    num_heads=num_heads,\n",
        "    window_size=window_size,\n",
        "    shift_size=shift_size,\n",
        "    num_mlp=num_mlp,\n",
        "    qkv_bias=qkv_bias,\n",
        "    dropout_rate=dropout_rate,\n",
        ")(x)\n",
        "x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "output = layers.Dense(2, activation=\"softmax\")(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "RFVR3ODp-7eL"
      },
      "outputs": [],
      "source": [
        "model = keras.Model(input, output)\n",
        "model.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "    optimizer=tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "    ],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjKsAF4G_r0C",
        "outputId": "585761cf-11e6-440a-aa5e-05f2738f9546"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 0.], dtype=float32)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_one_hot = to_categorical(y,2)\n",
        "y_one_hot[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 0.], dtype=float32)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_one_hot_test = to_categorical(y_test, 2)\n",
        "y_one_hot_test[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Iyp1Re9k_jnt"
      },
      "outputs": [],
      "source": [
        "X_train = X_train_padded.reshape(35000,10,10,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FJw6-Xv_Tm3",
        "outputId": "237c1203-3405-40c4-f5c7-b6436aefb7cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "274/274 [==============================] - 8s 21ms/step - loss: 736.3375 - accuracy: 0.4971 - top-5-accuracy: 1.0000\n",
            "Epoch 2/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 322.7798 - accuracy: 0.4995 - top-5-accuracy: 1.0000\n",
            "Epoch 3/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 199.8548 - accuracy: 0.4948 - top-5-accuracy: 1.0000\n",
            "Epoch 4/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 129.8005 - accuracy: 0.4993 - top-5-accuracy: 1.0000\n",
            "Epoch 5/300\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 79.7764 - accuracy: 0.5011 - top-5-accuracy: 1.0000\n",
            "Epoch 6/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 52.7401 - accuracy: 0.5044 - top-5-accuracy: 1.0000\n",
            "Epoch 7/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 37.0475 - accuracy: 0.5065 - top-5-accuracy: 1.0000\n",
            "Epoch 8/300\n",
            "274/274 [==============================] - 7s 24ms/step - loss: 26.4387 - accuracy: 0.5019 - top-5-accuracy: 1.0000\n",
            "Epoch 9/300\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 17.8739 - accuracy: 0.5038 - top-5-accuracy: 1.0000\n",
            "Epoch 10/300\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 14.6188 - accuracy: 0.4994 - top-5-accuracy: 1.0000\n",
            "Epoch 11/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 11.2543 - accuracy: 0.5094 - top-5-accuracy: 1.0000\n",
            "Epoch 12/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 9.0433 - accuracy: 0.5006 - top-5-accuracy: 1.0000\n",
            "Epoch 13/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 7.3139 - accuracy: 0.5015 - top-5-accuracy: 1.0000\n",
            "Epoch 14/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 6.7338 - accuracy: 0.5000 - top-5-accuracy: 1.0000\n",
            "Epoch 15/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 6.2115 - accuracy: 0.5027 - top-5-accuracy: 1.0000\n",
            "Epoch 16/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 5.2339 - accuracy: 0.4994 - top-5-accuracy: 1.0000\n",
            "Epoch 17/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 4.4403 - accuracy: 0.4975 - top-5-accuracy: 1.0000\n",
            "Epoch 18/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 3.8962 - accuracy: 0.5042 - top-5-accuracy: 1.0000\n",
            "Epoch 19/300\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 3.4201 - accuracy: 0.5051 - top-5-accuracy: 1.0000\n",
            "Epoch 20/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 3.0947 - accuracy: 0.5026 - top-5-accuracy: 1.0000\n",
            "Epoch 21/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 2.8341 - accuracy: 0.5013 - top-5-accuracy: 1.0000\n",
            "Epoch 22/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 2.2542 - accuracy: 0.4992 - top-5-accuracy: 1.0000\n",
            "Epoch 23/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 1.8608 - accuracy: 0.5021 - top-5-accuracy: 1.0000\n",
            "Epoch 24/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 1.6690 - accuracy: 0.5003 - top-5-accuracy: 1.0000\n",
            "Epoch 25/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 1.4771 - accuracy: 0.5047 - top-5-accuracy: 1.0000\n",
            "Epoch 26/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.2678 - accuracy: 0.5007 - top-5-accuracy: 1.0000\n",
            "Epoch 27/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 1.1566 - accuracy: 0.5068 - top-5-accuracy: 1.0000\n",
            "Epoch 28/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.0615 - accuracy: 0.5032 - top-5-accuracy: 1.0000\n",
            "Epoch 29/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.9757 - accuracy: 0.5058 - top-5-accuracy: 1.0000\n",
            "Epoch 30/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.9421 - accuracy: 0.5024 - top-5-accuracy: 1.0000\n",
            "Epoch 31/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.8763 - accuracy: 0.5008 - top-5-accuracy: 1.0000\n",
            "Epoch 32/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.8345 - accuracy: 0.5014 - top-5-accuracy: 1.0000\n",
            "Epoch 33/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7895 - accuracy: 0.4995 - top-5-accuracy: 1.0000\n",
            "Epoch 34/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7778 - accuracy: 0.5017 - top-5-accuracy: 1.0000\n",
            "Epoch 35/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7666 - accuracy: 0.4987 - top-5-accuracy: 1.0000\n",
            "Epoch 36/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7546 - accuracy: 0.5033 - top-5-accuracy: 1.0000\n",
            "Epoch 37/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7411 - accuracy: 0.4965 - top-5-accuracy: 1.0000\n",
            "Epoch 38/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7310 - accuracy: 0.5056 - top-5-accuracy: 1.0000\n",
            "Epoch 39/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7378 - accuracy: 0.5051 - top-5-accuracy: 1.0000\n",
            "Epoch 40/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7272 - accuracy: 0.5000 - top-5-accuracy: 1.0000\n",
            "Epoch 41/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7286 - accuracy: 0.5031 - top-5-accuracy: 1.0000\n",
            "Epoch 42/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7251 - accuracy: 0.5062 - top-5-accuracy: 1.0000\n",
            "Epoch 43/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7225 - accuracy: 0.5050 - top-5-accuracy: 1.0000\n",
            "Epoch 44/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7222 - accuracy: 0.5025 - top-5-accuracy: 1.0000\n",
            "Epoch 45/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7238 - accuracy: 0.5040 - top-5-accuracy: 1.0000\n",
            "Epoch 46/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7267 - accuracy: 0.5026 - top-5-accuracy: 1.0000\n",
            "Epoch 47/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7231 - accuracy: 0.4985 - top-5-accuracy: 1.0000\n",
            "Epoch 48/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7296 - accuracy: 0.5073 - top-5-accuracy: 1.0000\n",
            "Epoch 49/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7352 - accuracy: 0.5032 - top-5-accuracy: 1.0000\n",
            "Epoch 50/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7335 - accuracy: 0.5026 - top-5-accuracy: 1.0000\n",
            "Epoch 51/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7330 - accuracy: 0.5005 - top-5-accuracy: 1.0000\n",
            "Epoch 52/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7306 - accuracy: 0.5012 - top-5-accuracy: 1.0000\n",
            "Epoch 53/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7329 - accuracy: 0.5018 - top-5-accuracy: 1.0000\n",
            "Epoch 54/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7426 - accuracy: 0.4996 - top-5-accuracy: 1.0000\n",
            "Epoch 55/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7504 - accuracy: 0.4968 - top-5-accuracy: 1.0000\n",
            "Epoch 56/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7430 - accuracy: 0.5022 - top-5-accuracy: 1.0000\n",
            "Epoch 57/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7365 - accuracy: 0.5037 - top-5-accuracy: 1.0000\n",
            "Epoch 58/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7389 - accuracy: 0.5032 - top-5-accuracy: 1.0000\n",
            "Epoch 59/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7412 - accuracy: 0.5073 - top-5-accuracy: 1.0000\n",
            "Epoch 60/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7325 - accuracy: 0.5035 - top-5-accuracy: 1.0000\n",
            "Epoch 61/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7389 - accuracy: 0.4969 - top-5-accuracy: 1.0000\n",
            "Epoch 62/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7480 - accuracy: 0.5063 - top-5-accuracy: 1.0000\n",
            "Epoch 63/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7519 - accuracy: 0.5059 - top-5-accuracy: 1.0000\n",
            "Epoch 64/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7515 - accuracy: 0.5049 - top-5-accuracy: 1.0000\n",
            "Epoch 65/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7601 - accuracy: 0.5049 - top-5-accuracy: 1.0000\n",
            "Epoch 66/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7494 - accuracy: 0.5069 - top-5-accuracy: 1.0000\n",
            "Epoch 67/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7371 - accuracy: 0.5002 - top-5-accuracy: 1.0000\n",
            "Epoch 68/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7374 - accuracy: 0.5017 - top-5-accuracy: 1.0000\n",
            "Epoch 69/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7439 - accuracy: 0.5010 - top-5-accuracy: 1.0000\n",
            "Epoch 70/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7452 - accuracy: 0.5007 - top-5-accuracy: 1.0000\n",
            "Epoch 71/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7539 - accuracy: 0.5065 - top-5-accuracy: 1.0000\n",
            "Epoch 72/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7579 - accuracy: 0.5007 - top-5-accuracy: 1.0000\n",
            "Epoch 73/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.8034 - accuracy: 0.5037 - top-5-accuracy: 1.0000\n",
            "Epoch 74/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7667 - accuracy: 0.5049 - top-5-accuracy: 1.0000\n",
            "Epoch 75/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7303 - accuracy: 0.5038 - top-5-accuracy: 1.0000\n",
            "Epoch 76/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7213 - accuracy: 0.5022 - top-5-accuracy: 1.0000\n",
            "Epoch 77/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7235 - accuracy: 0.5025 - top-5-accuracy: 1.0000\n",
            "Epoch 78/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7379 - accuracy: 0.5023 - top-5-accuracy: 1.0000\n",
            "Epoch 79/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.8735 - accuracy: 0.4957 - top-5-accuracy: 1.0000\n",
            "Epoch 80/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.8183 - accuracy: 0.4999 - top-5-accuracy: 1.0000\n",
            "Epoch 81/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7251 - accuracy: 0.5003 - top-5-accuracy: 1.0000\n",
            "Epoch 82/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7117 - accuracy: 0.5005 - top-5-accuracy: 1.0000\n",
            "Epoch 83/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7125 - accuracy: 0.5055 - top-5-accuracy: 1.0000\n",
            "Epoch 84/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7269 - accuracy: 0.5067 - top-5-accuracy: 1.0000\n",
            "Epoch 85/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7335 - accuracy: 0.5031 - top-5-accuracy: 1.0000\n",
            "Epoch 86/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7503 - accuracy: 0.5020 - top-5-accuracy: 1.0000\n",
            "Epoch 87/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7733 - accuracy: 0.4981 - top-5-accuracy: 1.0000\n",
            "Epoch 88/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7950 - accuracy: 0.5034 - top-5-accuracy: 1.0000\n",
            "Epoch 89/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7581 - accuracy: 0.5061 - top-5-accuracy: 1.0000\n",
            "Epoch 90/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7301 - accuracy: 0.5000 - top-5-accuracy: 1.0000\n",
            "Epoch 91/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7220 - accuracy: 0.5053 - top-5-accuracy: 1.0000\n",
            "Epoch 92/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7350 - accuracy: 0.5017 - top-5-accuracy: 1.0000\n",
            "Epoch 93/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7421 - accuracy: 0.4996 - top-5-accuracy: 1.0000\n",
            "Epoch 94/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7698 - accuracy: 0.5005 - top-5-accuracy: 1.0000\n",
            "Epoch 95/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7757 - accuracy: 0.5021 - top-5-accuracy: 1.0000\n",
            "Epoch 96/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7662 - accuracy: 0.4973 - top-5-accuracy: 1.0000\n",
            "Epoch 97/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7296 - accuracy: 0.5038 - top-5-accuracy: 1.0000\n",
            "Epoch 98/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7261 - accuracy: 0.5051 - top-5-accuracy: 1.0000\n",
            "Epoch 99/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7294 - accuracy: 0.5011 - top-5-accuracy: 1.0000\n",
            "Epoch 100/300\n",
            "274/274 [==============================] - 6s 24ms/step - loss: 0.7515 - accuracy: 0.4961 - top-5-accuracy: 1.0000\n",
            "Epoch 101/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7524 - accuracy: 0.5014 - top-5-accuracy: 1.0000\n",
            "Epoch 102/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7848 - accuracy: 0.5026 - top-5-accuracy: 1.0000\n",
            "Epoch 103/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7414 - accuracy: 0.4956 - top-5-accuracy: 1.0000\n",
            "Epoch 104/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7343 - accuracy: 0.5031 - top-5-accuracy: 1.0000\n",
            "Epoch 105/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7381 - accuracy: 0.4990 - top-5-accuracy: 1.0000\n",
            "Epoch 106/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7365 - accuracy: 0.5052 - top-5-accuracy: 1.0000\n",
            "Epoch 107/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7457 - accuracy: 0.5052 - top-5-accuracy: 1.0000\n",
            "Epoch 108/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7444 - accuracy: 0.5031 - top-5-accuracy: 1.0000\n",
            "Epoch 109/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7533 - accuracy: 0.5019 - top-5-accuracy: 1.0000\n",
            "Epoch 110/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7820 - accuracy: 0.5024 - top-5-accuracy: 1.0000\n",
            "Epoch 111/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7397 - accuracy: 0.5048 - top-5-accuracy: 1.0000\n",
            "Epoch 112/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7279 - accuracy: 0.5055 - top-5-accuracy: 1.0000\n",
            "Epoch 113/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7298 - accuracy: 0.4981 - top-5-accuracy: 1.0000\n",
            "Epoch 114/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7506 - accuracy: 0.5037 - top-5-accuracy: 1.0000\n",
            "Epoch 115/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7863 - accuracy: 0.5051 - top-5-accuracy: 1.0000\n",
            "Epoch 116/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7558 - accuracy: 0.5006 - top-5-accuracy: 1.0000\n",
            "Epoch 117/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7369 - accuracy: 0.4980 - top-5-accuracy: 1.0000\n",
            "Epoch 118/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7240 - accuracy: 0.4978 - top-5-accuracy: 1.0000\n",
            "Epoch 119/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7319 - accuracy: 0.4992 - top-5-accuracy: 1.0000\n",
            "Epoch 120/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7379 - accuracy: 0.5003 - top-5-accuracy: 1.0000\n",
            "Epoch 121/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7792 - accuracy: 0.5083 - top-5-accuracy: 1.0000\n",
            "Epoch 122/300\n",
            "274/274 [==============================] - 5s 20ms/step - loss: 0.7446 - accuracy: 0.5008 - top-5-accuracy: 1.0000\n",
            "Epoch 123/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7320 - accuracy: 0.5004 - top-5-accuracy: 1.0000\n",
            "Epoch 124/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7418 - accuracy: 0.5057 - top-5-accuracy: 1.0000\n",
            "Epoch 125/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7392 - accuracy: 0.5043 - top-5-accuracy: 1.0000\n",
            "Epoch 126/300\n",
            "274/274 [==============================] - 6s 24ms/step - loss: 0.7463 - accuracy: 0.4992 - top-5-accuracy: 1.0000\n",
            "Epoch 127/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7652 - accuracy: 0.5008 - top-5-accuracy: 1.0000\n",
            "Epoch 128/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7423 - accuracy: 0.5017 - top-5-accuracy: 1.0000\n",
            "Epoch 129/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7439 - accuracy: 0.5039 - top-5-accuracy: 1.0000\n",
            "Epoch 130/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7304 - accuracy: 0.5008 - top-5-accuracy: 1.0000\n",
            "Epoch 131/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7336 - accuracy: 0.4996 - top-5-accuracy: 1.0000\n",
            "Epoch 132/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7421 - accuracy: 0.5083 - top-5-accuracy: 1.0000\n",
            "Epoch 133/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7625 - accuracy: 0.5004 - top-5-accuracy: 1.0000\n",
            "Epoch 134/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7438 - accuracy: 0.5038 - top-5-accuracy: 1.0000\n",
            "Epoch 135/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7332 - accuracy: 0.5029 - top-5-accuracy: 1.0000\n",
            "Epoch 136/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7376 - accuracy: 0.4976 - top-5-accuracy: 1.0000\n",
            "Epoch 137/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7320 - accuracy: 0.4974 - top-5-accuracy: 1.0000\n",
            "Epoch 138/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7411 - accuracy: 0.5007 - top-5-accuracy: 1.0000\n",
            "Epoch 139/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.8165 - accuracy: 0.4981 - top-5-accuracy: 1.0000\n",
            "Epoch 140/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7662 - accuracy: 0.5064 - top-5-accuracy: 1.0000\n",
            "Epoch 141/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7220 - accuracy: 0.5073 - top-5-accuracy: 1.0000\n",
            "Epoch 142/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7209 - accuracy: 0.5041 - top-5-accuracy: 1.0000\n",
            "Epoch 143/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7164 - accuracy: 0.5044 - top-5-accuracy: 1.0000\n",
            "Epoch 144/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7424 - accuracy: 0.5061 - top-5-accuracy: 1.0000\n",
            "Epoch 145/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7840 - accuracy: 0.5019 - top-5-accuracy: 1.0000\n",
            "Epoch 146/300\n",
            "274/274 [==============================] - 6s 24ms/step - loss: 0.8135 - accuracy: 0.5029 - top-5-accuracy: 1.0000\n",
            "Epoch 147/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7277 - accuracy: 0.4998 - top-5-accuracy: 1.0000\n",
            "Epoch 148/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7113 - accuracy: 0.5005 - top-5-accuracy: 1.0000\n",
            "Epoch 149/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7161 - accuracy: 0.5036 - top-5-accuracy: 1.0000\n",
            "Epoch 150/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7210 - accuracy: 0.5016 - top-5-accuracy: 1.0000\n",
            "Epoch 151/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7642 - accuracy: 0.5013 - top-5-accuracy: 1.0000\n",
            "Epoch 152/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7974 - accuracy: 0.4968 - top-5-accuracy: 1.0000\n",
            "Epoch 153/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7358 - accuracy: 0.5025 - top-5-accuracy: 1.0000\n",
            "Epoch 154/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7185 - accuracy: 0.4971 - top-5-accuracy: 1.0000\n",
            "Epoch 155/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7187 - accuracy: 0.4981 - top-5-accuracy: 1.0000\n",
            "Epoch 156/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7344 - accuracy: 0.5039 - top-5-accuracy: 1.0000\n",
            "Epoch 157/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.8054 - accuracy: 0.4983 - top-5-accuracy: 1.0000\n",
            "Epoch 158/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7368 - accuracy: 0.5002 - top-5-accuracy: 1.0000\n",
            "Epoch 159/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7192 - accuracy: 0.4981 - top-5-accuracy: 1.0000\n",
            "Epoch 160/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7194 - accuracy: 0.5060 - top-5-accuracy: 1.0000\n",
            "Epoch 161/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7335 - accuracy: 0.5032 - top-5-accuracy: 1.0000\n",
            "Epoch 162/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7864 - accuracy: 0.4977 - top-5-accuracy: 1.0000\n",
            "Epoch 163/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7431 - accuracy: 0.5025 - top-5-accuracy: 1.0000\n",
            "Epoch 164/300\n",
            "274/274 [==============================] - 11s 40ms/step - loss: 0.7238 - accuracy: 0.5081 - top-5-accuracy: 1.0000\n",
            "Epoch 165/300\n",
            "274/274 [==============================] - 10s 35ms/step - loss: 0.7235 - accuracy: 0.4997 - top-5-accuracy: 1.0000\n",
            "Epoch 166/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7223 - accuracy: 0.5074 - top-5-accuracy: 1.0000\n",
            "Epoch 167/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7513 - accuracy: 0.5033 - top-5-accuracy: 1.0000\n",
            "Epoch 168/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7751 - accuracy: 0.5015 - top-5-accuracy: 1.0000\n",
            "Epoch 169/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7355 - accuracy: 0.5003 - top-5-accuracy: 1.0000\n",
            "Epoch 170/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7239 - accuracy: 0.4935 - top-5-accuracy: 1.0000\n",
            "Epoch 171/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7248 - accuracy: 0.5020 - top-5-accuracy: 1.0000\n",
            "Epoch 172/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7321 - accuracy: 0.5024 - top-5-accuracy: 1.0000\n",
            "Epoch 173/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7789 - accuracy: 0.5023 - top-5-accuracy: 1.0000\n",
            "Epoch 174/300\n",
            "274/274 [==============================] - 6s 24ms/step - loss: 0.8207 - accuracy: 0.5014 - top-5-accuracy: 1.0000\n",
            "Epoch 175/300\n",
            "274/274 [==============================] - 9s 33ms/step - loss: 0.7329 - accuracy: 0.4959 - top-5-accuracy: 1.0000\n",
            "Epoch 176/300\n",
            "274/274 [==============================] - 8s 28ms/step - loss: 0.7133 - accuracy: 0.4995 - top-5-accuracy: 1.0000\n",
            "Epoch 177/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7092 - accuracy: 0.5011 - top-5-accuracy: 1.0000\n",
            "Epoch 178/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7163 - accuracy: 0.5003 - top-5-accuracy: 1.0000\n",
            "Epoch 179/300\n",
            "274/274 [==============================] - 9s 32ms/step - loss: 0.7726 - accuracy: 0.4995 - top-5-accuracy: 1.0000\n",
            "Epoch 180/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.8026 - accuracy: 0.4971 - top-5-accuracy: 1.0000\n",
            "Epoch 181/300\n",
            "274/274 [==============================] - 9s 31ms/step - loss: 0.7309 - accuracy: 0.4974 - top-5-accuracy: 1.0000\n",
            "Epoch 182/300\n",
            "274/274 [==============================] - 7s 24ms/step - loss: 0.7161 - accuracy: 0.4997 - top-5-accuracy: 1.0000\n",
            "Epoch 183/300\n",
            "274/274 [==============================] - 7s 27ms/step - loss: 0.7169 - accuracy: 0.4967 - top-5-accuracy: 1.0000\n",
            "Epoch 184/300\n",
            "274/274 [==============================] - 8s 29ms/step - loss: 0.7188 - accuracy: 0.5041 - top-5-accuracy: 1.0000\n",
            "Epoch 185/300\n",
            "274/274 [==============================] - 7s 24ms/step - loss: 0.7515 - accuracy: 0.5005 - top-5-accuracy: 1.0000\n",
            "Epoch 186/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.9606 - accuracy: 0.5009 - top-5-accuracy: 1.0000\n",
            "Epoch 187/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7331 - accuracy: 0.5058 - top-5-accuracy: 1.0000\n",
            "Epoch 188/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7090 - accuracy: 0.5009 - top-5-accuracy: 1.0000\n",
            "Epoch 189/300\n",
            "274/274 [==============================] - 9s 32ms/step - loss: 0.7073 - accuracy: 0.5033 - top-5-accuracy: 1.0000\n",
            "Epoch 190/300\n",
            "274/274 [==============================] - 13s 46ms/step - loss: 0.7058 - accuracy: 0.4991 - top-5-accuracy: 1.0000\n",
            "Epoch 191/300\n",
            "274/274 [==============================] - 13s 46ms/step - loss: 0.7098 - accuracy: 0.5024 - top-5-accuracy: 1.0000\n",
            "Epoch 192/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7285 - accuracy: 0.5030 - top-5-accuracy: 1.0000\n",
            "Epoch 193/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7744 - accuracy: 0.5018 - top-5-accuracy: 1.0000\n",
            "Epoch 194/300\n",
            "274/274 [==============================] - 9s 32ms/step - loss: 0.7964 - accuracy: 0.5007 - top-5-accuracy: 1.0000\n",
            "Epoch 195/300\n",
            "274/274 [==============================] - 8s 29ms/step - loss: 0.7223 - accuracy: 0.5041 - top-5-accuracy: 1.0000\n",
            "Epoch 196/300\n",
            "274/274 [==============================] - 8s 29ms/step - loss: 0.7151 - accuracy: 0.5046 - top-5-accuracy: 1.0000\n",
            "Epoch 197/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7149 - accuracy: 0.5004 - top-5-accuracy: 1.0000\n",
            "Epoch 198/300\n",
            "274/274 [==============================] - 8s 29ms/step - loss: 0.7229 - accuracy: 0.5028 - top-5-accuracy: 1.0000\n",
            "Epoch 199/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7302 - accuracy: 0.5001 - top-5-accuracy: 1.0000\n",
            "Epoch 200/300\n",
            "274/274 [==============================] - 8s 28ms/step - loss: 0.8967 - accuracy: 0.5019 - top-5-accuracy: 1.0000\n",
            "Epoch 201/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7788 - accuracy: 0.5028 - top-5-accuracy: 1.0000\n",
            "Epoch 202/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7167 - accuracy: 0.5006 - top-5-accuracy: 1.0000\n",
            "Epoch 203/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7112 - accuracy: 0.5014 - top-5-accuracy: 1.0000\n",
            "Epoch 204/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7111 - accuracy: 0.5059 - top-5-accuracy: 1.0000\n",
            "Epoch 205/300\n",
            "274/274 [==============================] - 6s 24ms/step - loss: 0.7142 - accuracy: 0.5000 - top-5-accuracy: 1.0000\n",
            "Epoch 206/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7252 - accuracy: 0.5023 - top-5-accuracy: 1.0000\n",
            "Epoch 207/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7844 - accuracy: 0.5026 - top-5-accuracy: 1.0000\n",
            "Epoch 208/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7639 - accuracy: 0.4984 - top-5-accuracy: 1.0000\n",
            "Epoch 209/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7265 - accuracy: 0.5001 - top-5-accuracy: 1.0000\n",
            "Epoch 210/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7252 - accuracy: 0.4999 - top-5-accuracy: 1.0000\n",
            "Epoch 211/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7216 - accuracy: 0.4988 - top-5-accuracy: 1.0000\n",
            "Epoch 212/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7281 - accuracy: 0.5051 - top-5-accuracy: 1.0000\n",
            "Epoch 213/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7627 - accuracy: 0.5026 - top-5-accuracy: 1.0000\n",
            "Epoch 214/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7748 - accuracy: 0.5006 - top-5-accuracy: 1.0000\n",
            "Epoch 215/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7475 - accuracy: 0.5015 - top-5-accuracy: 1.0000\n",
            "Epoch 216/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7242 - accuracy: 0.4966 - top-5-accuracy: 1.0000\n",
            "Epoch 217/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7228 - accuracy: 0.5056 - top-5-accuracy: 1.0000\n",
            "Epoch 218/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7327 - accuracy: 0.5003 - top-5-accuracy: 1.0000\n",
            "Epoch 219/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7721 - accuracy: 0.5012 - top-5-accuracy: 1.0000\n",
            "Epoch 220/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7640 - accuracy: 0.5037 - top-5-accuracy: 1.0000\n",
            "Epoch 221/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7377 - accuracy: 0.4975 - top-5-accuracy: 1.0000\n",
            "Epoch 222/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7244 - accuracy: 0.4990 - top-5-accuracy: 1.0000\n",
            "Epoch 223/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7273 - accuracy: 0.4990 - top-5-accuracy: 1.0000\n",
            "Epoch 224/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7451 - accuracy: 0.5019 - top-5-accuracy: 1.0000\n",
            "Epoch 225/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7763 - accuracy: 0.4957 - top-5-accuracy: 1.0000\n",
            "Epoch 226/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7514 - accuracy: 0.5016 - top-5-accuracy: 1.0000\n",
            "Epoch 227/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7227 - accuracy: 0.5036 - top-5-accuracy: 1.0000\n",
            "Epoch 228/300\n",
            "274/274 [==============================] - 9s 32ms/step - loss: 0.7289 - accuracy: 0.5017 - top-5-accuracy: 1.0000\n",
            "Epoch 229/300\n",
            "274/274 [==============================] - 8s 27ms/step - loss: 0.7358 - accuracy: 0.5013 - top-5-accuracy: 1.0000\n",
            "Epoch 230/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7475 - accuracy: 0.5040 - top-5-accuracy: 1.0000\n",
            "Epoch 231/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7882 - accuracy: 0.5068 - top-5-accuracy: 1.0000\n",
            "Epoch 232/300\n",
            "274/274 [==============================] - 8s 30ms/step - loss: 0.7426 - accuracy: 0.5040 - top-5-accuracy: 1.0000\n",
            "Epoch 233/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7182 - accuracy: 0.5000 - top-5-accuracy: 1.0000\n",
            "Epoch 234/300\n",
            "274/274 [==============================] - 8s 28ms/step - loss: 0.7161 - accuracy: 0.4997 - top-5-accuracy: 1.0000\n",
            "Epoch 235/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7233 - accuracy: 0.5011 - top-5-accuracy: 1.0000\n",
            "Epoch 236/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7576 - accuracy: 0.5003 - top-5-accuracy: 1.0000\n",
            "Epoch 237/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.8040 - accuracy: 0.5012 - top-5-accuracy: 1.0000\n",
            "Epoch 238/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7485 - accuracy: 0.4945 - top-5-accuracy: 1.0000\n",
            "Epoch 239/300\n",
            "274/274 [==============================] - 9s 32ms/step - loss: 0.7156 - accuracy: 0.4991 - top-5-accuracy: 1.0000\n",
            "Epoch 240/300\n",
            "274/274 [==============================] - 6s 24ms/step - loss: 0.7166 - accuracy: 0.5008 - top-5-accuracy: 1.0000\n",
            "Epoch 241/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7253 - accuracy: 0.5020 - top-5-accuracy: 1.0000\n",
            "Epoch 242/300\n",
            "274/274 [==============================] - 8s 28ms/step - loss: 0.7635 - accuracy: 0.4997 - top-5-accuracy: 1.0000\n",
            "Epoch 243/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7762 - accuracy: 0.5011 - top-5-accuracy: 1.0000\n",
            "Epoch 244/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7355 - accuracy: 0.5047 - top-5-accuracy: 1.0000\n",
            "Epoch 245/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7259 - accuracy: 0.5017 - top-5-accuracy: 1.0000\n",
            "Epoch 246/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7256 - accuracy: 0.4980 - top-5-accuracy: 1.0000\n",
            "Epoch 247/300\n",
            "274/274 [==============================] - 7s 27ms/step - loss: 0.7331 - accuracy: 0.5059 - top-5-accuracy: 1.0000\n",
            "Epoch 248/300\n",
            "274/274 [==============================] - 7s 27ms/step - loss: 0.7403 - accuracy: 0.5044 - top-5-accuracy: 1.0000\n",
            "Epoch 249/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7442 - accuracy: 0.5044 - top-5-accuracy: 1.0000\n",
            "Epoch 250/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7562 - accuracy: 0.5036 - top-5-accuracy: 1.0000\n",
            "Epoch 251/300\n",
            "274/274 [==============================] - 8s 29ms/step - loss: 0.7775 - accuracy: 0.4998 - top-5-accuracy: 1.0000\n",
            "Epoch 252/300\n",
            "274/274 [==============================] - 7s 24ms/step - loss: 0.7274 - accuracy: 0.4995 - top-5-accuracy: 1.0000\n",
            "Epoch 253/300\n",
            "274/274 [==============================] - 8s 28ms/step - loss: 0.7187 - accuracy: 0.5037 - top-5-accuracy: 1.0000\n",
            "Epoch 254/300\n",
            "274/274 [==============================] - 7s 27ms/step - loss: 0.7176 - accuracy: 0.5021 - top-5-accuracy: 1.0000\n",
            "Epoch 255/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7315 - accuracy: 0.5035 - top-5-accuracy: 1.0000\n",
            "Epoch 256/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7461 - accuracy: 0.5014 - top-5-accuracy: 1.0000\n",
            "Epoch 257/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7927 - accuracy: 0.5031 - top-5-accuracy: 1.0000\n",
            "Epoch 258/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7559 - accuracy: 0.5042 - top-5-accuracy: 1.0000\n",
            "Epoch 259/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7220 - accuracy: 0.5009 - top-5-accuracy: 1.0000\n",
            "Epoch 260/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7218 - accuracy: 0.5000 - top-5-accuracy: 1.0000\n",
            "Epoch 261/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7225 - accuracy: 0.5003 - top-5-accuracy: 1.0000\n",
            "Epoch 262/300\n",
            "274/274 [==============================] - 7s 27ms/step - loss: 0.7423 - accuracy: 0.5032 - top-5-accuracy: 1.0000\n",
            "Epoch 263/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7537 - accuracy: 0.4997 - top-5-accuracy: 1.0000\n",
            "Epoch 264/300\n",
            "274/274 [==============================] - 6s 24ms/step - loss: 0.7353 - accuracy: 0.5039 - top-5-accuracy: 1.0000\n",
            "Epoch 265/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7540 - accuracy: 0.5055 - top-5-accuracy: 1.0000\n",
            "Epoch 266/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7330 - accuracy: 0.5026 - top-5-accuracy: 1.0000\n",
            "Epoch 267/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7407 - accuracy: 0.5047 - top-5-accuracy: 1.0000\n",
            "Epoch 268/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7749 - accuracy: 0.5023 - top-5-accuracy: 1.0000\n",
            "Epoch 269/300\n",
            "274/274 [==============================] - 8s 28ms/step - loss: 0.7280 - accuracy: 0.5025 - top-5-accuracy: 1.0000\n",
            "Epoch 270/300\n",
            "274/274 [==============================] - 7s 24ms/step - loss: 0.7262 - accuracy: 0.4991 - top-5-accuracy: 1.0000\n",
            "Epoch 271/300\n",
            "274/274 [==============================] - 7s 24ms/step - loss: 0.7228 - accuracy: 0.4946 - top-5-accuracy: 1.0000\n",
            "Epoch 272/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7638 - accuracy: 0.5022 - top-5-accuracy: 1.0000\n",
            "Epoch 273/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.9030 - accuracy: 0.4997 - top-5-accuracy: 1.0000\n",
            "Epoch 274/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7175 - accuracy: 0.4993 - top-5-accuracy: 1.0000\n",
            "Epoch 275/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7144 - accuracy: 0.5050 - top-5-accuracy: 1.0000\n",
            "Epoch 276/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7107 - accuracy: 0.5049 - top-5-accuracy: 1.0000\n",
            "Epoch 277/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7124 - accuracy: 0.4985 - top-5-accuracy: 1.0000\n",
            "Epoch 278/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7158 - accuracy: 0.5020 - top-5-accuracy: 1.0000\n",
            "Epoch 279/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7367 - accuracy: 0.5011 - top-5-accuracy: 1.0000\n",
            "Epoch 280/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7861 - accuracy: 0.4977 - top-5-accuracy: 1.0000\n",
            "Epoch 281/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7515 - accuracy: 0.4975 - top-5-accuracy: 1.0000\n",
            "Epoch 282/300\n",
            "274/274 [==============================] - 6s 24ms/step - loss: 0.7287 - accuracy: 0.4984 - top-5-accuracy: 1.0000\n",
            "Epoch 283/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7229 - accuracy: 0.4984 - top-5-accuracy: 1.0000\n",
            "Epoch 284/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7334 - accuracy: 0.5037 - top-5-accuracy: 1.0000\n",
            "Epoch 285/300\n",
            "274/274 [==============================] - 6s 24ms/step - loss: 0.7538 - accuracy: 0.5034 - top-5-accuracy: 1.0000\n",
            "Epoch 286/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7442 - accuracy: 0.5048 - top-5-accuracy: 1.0000\n",
            "Epoch 287/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7601 - accuracy: 0.5016 - top-5-accuracy: 1.0000\n",
            "Epoch 288/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7295 - accuracy: 0.5021 - top-5-accuracy: 1.0000\n",
            "Epoch 289/300\n",
            "274/274 [==============================] - 7s 25ms/step - loss: 0.7284 - accuracy: 0.4982 - top-5-accuracy: 1.0000\n",
            "Epoch 290/300\n",
            "274/274 [==============================] - 7s 24ms/step - loss: 0.7296 - accuracy: 0.4991 - top-5-accuracy: 1.0000\n",
            "Epoch 291/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7661 - accuracy: 0.5022 - top-5-accuracy: 1.0000\n",
            "Epoch 292/300\n",
            "274/274 [==============================] - 7s 26ms/step - loss: 0.7409 - accuracy: 0.5000 - top-5-accuracy: 1.0000\n",
            "Epoch 293/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7311 - accuracy: 0.5021 - top-5-accuracy: 1.0000\n",
            "Epoch 294/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7254 - accuracy: 0.4973 - top-5-accuracy: 1.0000\n",
            "Epoch 295/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7203 - accuracy: 0.5047 - top-5-accuracy: 1.0000\n",
            "Epoch 296/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7593 - accuracy: 0.5033 - top-5-accuracy: 1.0000\n",
            "Epoch 297/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7692 - accuracy: 0.5003 - top-5-accuracy: 1.0000\n",
            "Epoch 298/300\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.7310 - accuracy: 0.5010 - top-5-accuracy: 1.0000\n",
            "Epoch 299/300\n",
            "274/274 [==============================] - 6s 23ms/step - loss: 0.7257 - accuracy: 0.5052 - top-5-accuracy: 1.0000\n",
            "Epoch 300/300\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 0.7263 - accuracy: 0.5027 - top-5-accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_one_hot,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = X_test_padded.reshape(15000,10,10,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "JS8Lb5oqAYpK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 2s 3ms/step - loss: 0.7182 - accuracy: 0.4897 - top-5-accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.7181634902954102, 0.48973333835601807, 1.0]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(X_test, y_one_hot_test )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "workspace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3b25f1362c139dd4858e03e9e72a73056e3802632b8fbc2e6341212b824376d8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
